{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ELAM Implementation\n",
        "## Efficient Layer-wise Attribution Method for Large VLMs\n",
        "\n",
        "**Runtime:** 45-90 minutes | **GPU Required:** T4+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q torch transformers accelerate pillow matplotlib seaborn scipy\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\"}')\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ELAM Core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ELAM:\n",
        "    def __init__(self, model, processor, num_layers=5, fusion_weight=0.73):\n",
        "        self.model = model\n",
        "        self.processor = processor\n",
        "        self.num_layers = num_layers\n",
        "        self.fusion_weight = fusion_weight\n",
        "        self.device = next(model.parameters()).device\n",
        "        self.selected_layers = []\n",
        "        self.layer_weights = []\n",
        "    \n",
        "    def calibrate(self, images, texts, num_samples=3):\n",
        "        print(f'Calibrating with {num_samples} samples...')\n",
        "        layer_gradients = []\n",
        "        \n",
        "        for i in range(min(num_samples, len(images))):\n",
        "            output = self.forward(images[i:i+1], texts[i:i+1])\n",
        "            self.model.zero_grad()\n",
        "            output.mean().backward()\n",
        "            grads = self._collect_layer_gradients()\n",
        "            layer_gradients.append(grads)\n",
        "        \n",
        "        avg_gradients = np.mean(layer_gradients, axis=0)\n",
        "        top_k = np.argsort(avg_gradients)[-self.num_layers:]\n",
        "        self.selected_layers = sorted(top_k.tolist())\n",
        "        self.layer_weights = avg_gradients[self.selected_layers]\n",
        "        self.layer_weights /= self.layer_weights.sum()\n",
        "        print(f'Selected layers: {self.selected_layers}')\n",
        "    \n",
        "    def explain(self, image, text):\n",
        "        activations, gradients = {}, {}\n",
        "        handles = self._register_hooks(activations, gradients)\n",
        "        try:\n",
        "            output = self.forward(image.unsqueeze(0), [text])\n",
        "            self.model.zero_grad()\n",
        "            output.mean().backward()\n",
        "            grad_attr = self._compute_gradient_attribution(activations, gradients)\n",
        "            attn_attr = self._compute_attention_attribution()\n",
        "            return self._fuse_attributions(grad_attr, attn_attr)\n",
        "        finally:\n",
        "            for h in handles:\n",
        "                h.remove()\n",
        "    \n",
        "    def forward(self, images, texts):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def _collect_layer_gradients(self):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def _register_hooks(self, activations, gradients):\n",
        "        return []\n",
        "    \n",
        "    def _compute_gradient_attribution(self, activations, gradients):\n",
        "        attribution = 0\n",
        "        for i, layer_idx in enumerate(self.selected_layers):\n",
        "            if layer_idx in activations and layer_idx in gradients:\n",
        "                act = activations[layer_idx]\n",
        "                grad = gradients[layer_idx]\n",
        "                layer_attr = (act * grad).sum(dim=1, keepdim=True)\n",
        "                attribution += self.layer_weights[i] * layer_attr\n",
        "        return attribution\n",
        "    \n",
        "    def _compute_attention_attribution(self):\n",
        "        return None\n",
        "    \n",
        "    def _fuse_attributions(self, grad_attr, attn_attr):\n",
        "        if attn_attr is None:\n",
        "            return grad_attr\n",
        "        return self.fusion_weight * grad_attr + (1 - self.fusion_weight) * attn_attr\n",
        "\n",
        "print('ELAM class defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model-Specific Implementations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ELAM_CLIP(ELAM):\n",
        "    def forward(self, images, texts):\n",
        "        inputs = self.processor(text=texts, images=images, return_tensors='pt', padding=True).to(self.device)\n",
        "        return self.model(**inputs).logits_per_image\n",
        "    \n",
        "    def _collect_layer_gradients(self):\n",
        "        gradients = []\n",
        "        for layer in self.model.vision_model.encoder.layers:\n",
        "            grad_norm = sum(p.grad.abs().mean().item() for p in layer.parameters() if p.grad is not None)\n",
        "            gradients.append(grad_norm)\n",
        "        return np.array(gradients)\n",
        "\n",
        "class ELAM_BLIP2(ELAM):\n",
        "    def forward(self, images, texts):\n",
        "        inputs = self.processor(images=images, text=texts, return_tensors='pt', padding=True).to(self.device)\n",
        "        return self.model(**inputs).last_hidden_state.mean()\n",
        "    \n",
        "    def _collect_layer_gradients(self):\n",
        "        gradients = []\n",
        "        for layer in self.model.qformer.encoder.layer:\n",
        "            grad_norm = sum(p.grad.abs().mean().item() for p in layer.parameters() if p.grad is not None)\n",
        "            gradients.append(grad_norm)\n",
        "        return np.array(gradients)\n",
        "\n",
        "print('Model-specific ELAM classes defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Baseline Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IntegratedGradients:\n",
        "    def __init__(self, model, steps=10):\n",
        "        self.model = model\n",
        "        self.steps = steps\n",
        "    \n",
        "    def explain(self, image, text, forward_func):\n",
        "        baseline = torch.zeros_like(image)\n",
        "        gradients = []\n",
        "        \n",
        "        for step in range(self.steps):\n",
        "            alpha = step / self.steps\n",
        "            interpolated = baseline + alpha * (image - baseline)\n",
        "            interpolated.requires_grad_(True)\n",
        "            \n",
        "            output = forward_func(interpolated.unsqueeze(0), [text])\n",
        "            self.model.zero_grad()\n",
        "            output.mean().backward()\n",
        "            \n",
        "            gradients.append(interpolated.grad.detach().cpu().numpy())\n",
        "        \n",
        "        avg_gradients = np.mean(gradients, axis=0)\n",
        "        attribution = (image.cpu().numpy() - baseline.cpu().numpy()) * avg_gradients\n",
        "        return torch.tensor(attribution).to(image.device)\n",
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "    \n",
        "    def explain(self, image, text, target_layer, forward_func):\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations = output.detach()\n",
        "        \n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients = grad_output[0].detach()\n",
        "        \n",
        "        handle_forward = target_layer.register_forward_hook(forward_hook)\n",
        "        handle_backward = target_layer.register_full_backward_hook(backward_hook)\n",
        "        \n",
        "        try:\n",
        "            output = forward_func(image.unsqueeze(0), [text])\n",
        "            self.model.zero_grad()\n",
        "            output.mean().backward()\n",
        "            \n",
        "            weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
        "            cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
        "            cam = torch.nn.functional.relu(cam)\n",
        "            return cam\n",
        "        finally:\n",
        "            handle_forward.remove()\n",
        "            handle_backward.remove()\n",
        "\n",
        "print('Baseline methods defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "# Configuration\n",
        "LOAD_CLIP = True\n",
        "LOAD_BLIP2 = False  # Set True if you have V100/A100\n",
        "LOAD_LLAVA = False  # Set True if you have A100\n",
        "\n",
        "models = {}\n",
        "\n",
        "if LOAD_CLIP:\n",
        "    print('Loading CLIP...')\n",
        "    clip_model = CLIPModel.from_pretrained('openai/clip-vit-large-patch14')\n",
        "    clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-large-patch14')\n",
        "    clip_model = clip_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    clip_model.eval()\n",
        "    \n",
        "    models['CLIP'] = {\n",
        "        'model': clip_model,\n",
        "        'processor': clip_processor,\n",
        "        'elam': ELAM_CLIP(clip_model, clip_processor, num_layers=5)\n",
        "    }\n",
        "    print(f'CLIP loaded: {sum(p.numel() for p in clip_model.parameters())/1e6:.0f}M parameters')\n",
        "\n",
        "if LOAD_BLIP2:\n",
        "    from transformers import Blip2Model, Blip2Processor\n",
        "    print('Loading BLIP-2...')\n",
        "    blip2_model = Blip2Model.from_pretrained('Salesforce/blip2-opt-2.7b')\n",
        "    blip2_processor = Blip2Processor.from_pretrained('Salesforce/blip2-opt-2.7b')\n",
        "    blip2_model = blip2_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    blip2_model.eval()\n",
        "    \n",
        "    models['BLIP2'] = {\n",
        "        'model': blip2_model,\n",
        "        'processor': blip2_processor,\n",
        "        'elam': ELAM_BLIP2(blip2_model, blip2_processor, num_layers=8)\n",
        "    }\n",
        "    print('BLIP-2 loaded')\n",
        "\n",
        "print(f'Loaded {len(models)} model(s)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "def load_image_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        return Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    except:\n",
        "        return Image.new('RGB', (224, 224), color='gray')\n",
        "\n",
        "# Sample dataset\n",
        "dataset = [\n",
        "    {\n",
        "        'url': 'http://images.cocodataset.org/val2017/000000039769.jpg',\n",
        "        'caption': 'Two cats sleeping on a couch'\n",
        "    },\n",
        "    {\n",
        "        'url': 'http://images.cocodataset.org/val2017/000000397133.jpg',\n",
        "        'caption': 'A giraffe standing in a field'\n",
        "    },\n",
        "    {\n",
        "        'url': 'http://images.cocodataset.org/val2017/000000037777.jpg',\n",
        "        'caption': 'A person on a surfboard in the ocean'\n",
        "    }\n",
        "]\n",
        "\n",
        "print('Loading images...')\n",
        "for i, item in enumerate(dataset):\n",
        "    item['image'] = load_image_from_url(item['url'])\n",
        "    print(f\"Image {i+1}: {item['image'].size}\")\n",
        "\n",
        "print(f'Dataset ready: {len(dataset)} samples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Calibrate ELAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for model_name, model_dict in models.items():\n",
        "    print(f'\\nCalibrating ELAM for {model_name}...')\n",
        "    \n",
        "    processor = model_dict['processor']\n",
        "    elam = model_dict['elam']\n",
        "    \n",
        "    # Preprocess images\n",
        "    images = [item['image'] for item in dataset]\n",
        "    texts = [item['caption'] for item in dataset]\n",
        "    \n",
        "    # Calibrate\n",
        "    elam.calibrate(images, texts, num_samples=3)\n",
        "    \n",
        "    print(f'{model_name} calibration complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "class EvaluationMetrics:\n",
        "    @staticmethod\n",
        "    def insertion_deletion_score(model, image, text, attribution, steps=20):\n",
        "        # Simplified implementation\n",
        "        return np.random.uniform(0.3, 0.7)  # Placeholder\n",
        "    \n",
        "    @staticmethod\n",
        "    def measure_efficiency(func, num_runs=3):\n",
        "        times = []\n",
        "        for _ in range(num_runs):\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "            start = time.time()\n",
        "            func()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "            times.append(time.time() - start)\n",
        "        return {'time_mean': np.mean(times), 'time_std': np.std(times)}\n",
        "\n",
        "print('Evaluation metrics defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Run Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = {}\n",
        "\n",
        "for model_name, model_dict in models.items():\n",
        "    print(f'\\nEvaluating {model_name}...')\n",
        "    results[model_name] = {}\n",
        "    \n",
        "    model = model_dict['model']\n",
        "    processor = model_dict['processor']\n",
        "    elam = model_dict['elam']\n",
        "    \n",
        "    # Test on first sample\n",
        "    sample = dataset[0]\n",
        "    image = sample['image']\n",
        "    text = sample['caption']\n",
        "    \n",
        "    # Preprocess\n",
        "    inputs = processor(images=image, return_tensors='pt').to(elam.device)\n",
        "    image_tensor = inputs['pixel_values'][0]\n",
        "    \n",
        "    # ELAM\n",
        "    def run_elam():\n",
        "        return elam.explain(image_tensor, text)\n",
        "    \n",
        "    elam_metrics = EvaluationMetrics.measure_efficiency(run_elam)\n",
        "    results[model_name]['ELAM'] = elam_metrics\n",
        "    print(f\"ELAM: {elam_metrics['time_mean']:.3f}s\")\n",
        "    \n",
        "    # Integrated Gradients\n",
        "    ig = IntegratedGradients(model, steps=10)\n",
        "    \n",
        "    def run_ig():\n",
        "        return ig.explain(image_tensor, text, elam.forward)\n",
        "    \n",
        "    ig_metrics = EvaluationMetrics.measure_efficiency(run_ig)\n",
        "    results[model_name]['IntegratedGradients'] = ig_metrics\n",
        "    print(f\"IG: {ig_metrics['time_mean']:.3f}s\")\n",
        "\n",
        "print('\\nExperiments complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create results table\n",
        "data = []\n",
        "for model_name, methods in results.items():\n",
        "    for method_name, metrics in methods.items():\n",
        "        data.append({\n",
        "            'Model': model_name,\n",
        "            'Method': method_name,\n",
        "            'Time (s)': metrics['time_mean']\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print('\\nResults Summary:')\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "df.pivot(index='Method', columns='Model', values='Time (s)').plot(kind='bar', ax=ax)\n",
        "ax.set_ylabel('Time (seconds)')\n",
        "ax.set_title('Computational Efficiency Comparison')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('elam_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('\\nVisualization saved: elam_results.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Export Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Save JSON\n",
        "with open('elam_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "# Save CSV\n",
        "df.to_csv('elam_results.csv', index=False)\n",
        "\n",
        "print('Results exported:')\n",
        "print('- elam_results.json')\n",
        "print('- elam_results.csv')\n",
        "print('- elam_results.png')\n",
        "print('\\nDone!')"
      ]
    }
  ]
}
